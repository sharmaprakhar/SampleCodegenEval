{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to establish the appropriate environment\n",
    "# !pip install torch transformers datasets accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up packages and hyperparams \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "import traceback\n",
    "\n",
    "models = [\n",
    "    \"bigcode/starcoder2-15b\",\n",
    "    \"codellama/CodeLlama-34b-Instruct-hf\",\n",
    "    \"deepseek-ai/deepseek-coder-33b-instruct\",\n",
    "    \"WizardLM/WizardCoder-Python-34B-V1.0\",\n",
    "    \"replit/replit-code-v1.5-3b\"\n",
    "] # can add or remove from this list\n",
    "\n",
    "dataset_name = \"openai_humaneval\" # can change this or add more\n",
    "max_new_tokens = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to validate the results\n",
    "\n",
    "def safe_exec(code_string, inputs=(), expected_output=None):\n",
    "    try:\n",
    "        local_env = {}\n",
    "        exec(code_string, {}, local_env)\n",
    "        func = [v for v in local_env.values() if callable(v)][0]\n",
    "        output = func(*inputs)\n",
    "        return output == expected_output\n",
    "    except Exception as e:\n",
    "        print(\"Execution error:\", e)\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run evaluation of LLM generated code - uses ast\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        prompt = sample[\"prompt\"]\n",
    "        test_code = sample[\"test\"]\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "        generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_code = generated_code[len(prompt):]\n",
    "\n",
    "        try:\n",
    "            parsed_test = ast.parse(test_code)\n",
    "            tests = [node for node in parsed_test.body if isinstance(node, ast.Assert)]\n",
    "\n",
    "            all_passed = True\n",
    "            for test in tests:\n",
    "                expr = ast.unparse(test.test)\n",
    "                fake_test_code = f\"{generated_code}\\nassert {expr}\"\n",
    "                passed = safe_exec(fake_test_code)\n",
    "                if not passed:\n",
    "                    all_passed = False\n",
    "                    break\n",
    "\n",
    "            if all_passed:\n",
    "                n_correct += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Test parsing error:\", e)\n",
    "\n",
    "        n_total += 1\n",
    "\n",
    "        if n_total % 5 == 0:\n",
    "            print(f\"Samples evaluated: {n_total} - Current Pass@1: {n_correct}/{n_total} = {n_correct/n_total:.2f}\")\n",
    "\n",
    "        # remove the following condition to run on the whole dataset\n",
    "        if n_total >= 20:\n",
    "            break\n",
    "\n",
    "    final_score = n_correct / n_total\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "# Quantization for low GPU resources. Remove to run full precision inference\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# for each llm in the list defined above\n",
    "for model_id in models:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate\n",
    "    score = evaluate_model(model, tokenizer, dataset)\n",
    "    print(f\"\\nâœ… {model_id} achieved Pass@1 score: {score:.4f} on first 20 samples.\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
